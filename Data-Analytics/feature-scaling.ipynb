{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "---\n",
    "- Author: Diego In√°cio\n",
    "- GitHub: [github.com/diegoinacio](https://github.com/diegoinacio)\n",
    "- Notebook: [feature-scaling.ipynb](https://github.com/diegoinacio/data-science-notebooks/blob/master/data-analytics/feature-scaling.ipynb)\n",
    "---\n",
    "Overview and practical applications of key *feature scaling* methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model (SVM Classification)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data\n",
    "---\n",
    "Before we start talking let's first start acquiring and preparing our data to run a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from Kaggle\n",
    "---\n",
    "For this experiment, we firstly are going to download the [diamond prices](https://www.kaggle.com/datasets/shivam2503/diamonds) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d \"shivam2503/diamonds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"diamonds.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "---\n",
    "Prepare our dataset to run a classification model based on numeric independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_diamonds = pd.read_csv(\"diamonds.csv\")\n",
    "df_diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diamonds = df_diamonds.drop(['Unnamed: 0'],axis=1)\n",
    "df_diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diamonds.groupby(\"color\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data and get just 1 categorical feature\n",
    "df_classification = (df_diamonds\n",
    "    .where(df_diamonds.color == \"G\")\n",
    "    .where(df_diamonds.clarity == \"VS2\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Numerical dependent features\n",
    "X = df_classification[[\n",
    "    \"carat\", \"depth\", \"table\", \n",
    "    \"price\", \"x\", \"y\", \"z\"\n",
    "]]\n",
    "\n",
    "# Target data (classification)\n",
    "y = df_classification[\"cut\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is *feature scaling*?\n",
    "---\n",
    "*Feature scaling* is a process used to rescale and normalize independent variables. This is an important process because sometimes columns can have different units and this can impact the performance of many algorithms that are based on dissimilarity between variables.\n",
    "\n",
    "For example, if we have a classifier based on distance metrics (like KNN, SVM and etc), our model may not work correctly if our variables have different ranges in terms of magnitude. Let's se how our model performs without applying feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split model data\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(X, y, random_state=0, test_size=0.1)\n",
    ")\n",
    "\n",
    "LABELS = y_train.unique()\n",
    "\n",
    "# Model | SVC Classification\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "CM = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=CM,display_labels=LABELS)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Normalization\n",
    "---\n",
    "Normalization is probably the most common method for scaling features. This is based on *minimums* and *maximums*, and adjusts all variables $X$ to the range $[0, 1]$.\n",
    "\n",
    "$$ \\large\n",
    "x' = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is an independent variable;\n",
    "- $min(x)$ is the minimum value of $x$;\n",
    "- $max(x)$ is the maximum value of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (X - X.min())/(X.max() - X.min())\n",
    "X_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0, test_size=0.1)\n",
    "\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=CM,display_labels=LABELS)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can redimension the interval of our variable to another range $[a, b]$.\n",
    "\n",
    "$$ \\large\n",
    "x' = a + (b - a) \\cdot \\frac{x - min(x)}{max(x) - min(x)}\n",
    "$$\n",
    "\n",
    "where $a$ and $b$ are the left and right limit of the range, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 1, 5\n",
    "X_ = a + (b - a)*(X - X.min())/(X.max() - X.min())\n",
    "X_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0, test_size=0.1)\n",
    "\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=CM,display_labels=LABELS)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Mean Normalization\n",
    "---\n",
    "This is similar to simple normalization, but is centrilized to the origin.\n",
    "\n",
    "$$ \\large\n",
    "x' = \\frac{x - \\overline{x}}{max(x) - min(x)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\overline{x}$ is the average (or arithmetic mean) value of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (X - X.mean())/(X.max() - X.min())\n",
    "X_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0, test_size=0.1)\n",
    "\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=CM,display_labels=LABELS)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. InterQuartile Range Normalization\n",
    "---\n",
    "Normalization is very sensitive to outliers, which can affect minimum and maximum values. It can stretch our feature a little. To make make it not sensitive to outliers we can use some measure of position values to normalize our data. So instead of minimum and maximum, let's take an *InterQuartile Range* (IQR):\n",
    "\n",
    "$$ \\large\n",
    "x' = \\frac{x - median(x)}{Q3 - Q1}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $median(x)$ is the median;\n",
    "- $Q1$ is the first quartile;\n",
    "- $Q3$ is the third quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "X_ = (X - X.median())/(Q3 - Q1)\n",
    "X_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0, test_size=0.1)\n",
    "\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=CM,display_labels=LABELS)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Standardization\n",
    "---\n",
    "This is another very popular scaler and really good to deal with outliers. this method is centrilized and weighted by *standard deviation*.\n",
    "\n",
    "$$ \\large\n",
    "x' = \\frac{x - \\overline{x}}{\\sigma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\overline{x}$ is the mean value;\n",
    "- $\\sigma$ is the standard deviation of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (X - X.mean())/X.std()\n",
    "X_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0, test_size=0.1)\n",
    "\n",
    "svc = SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "CM = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=CM,display_labels=LABELS)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=LABELS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
